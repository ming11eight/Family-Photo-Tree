{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from data_loader import get_dataloader\n",
    "from datasets.write_csv_for_making_dataset import write_csv\n",
    "from eval_metrics import evaluate, plot_roc\n",
    "from loss import TripletLoss\n",
    "from models import FaceNetModel\n",
    "from utils import ModelSaver, init_log_just_created\n",
    "from inception_resnet_v1 import InceptionResnetV1\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Face Recognition using Triplet Loss')\n",
    "\n",
    "parser.add_argument('--num-epochs', default=50, type=int, metavar='NE',\n",
    "                    help='number of epochs to train (default: 200)')\n",
    "parser.add_argument('--num-train-triplets', default=100, type=int, metavar='NTT',\n",
    "                    help='number of triplets for training (default: 10000)')\n",
    "parser.add_argument('--num-valid-triplets', default=100, type=int, metavar='NVT',\n",
    "                    help='number of triplets for vaidation (default: 10000)')\n",
    "parser.add_argument('--batch-size', default=16, type=int, metavar='BS',\n",
    "                    help='batch size (default: 128)')\n",
    "parser.add_argument('--num-workers', default=8, type=int, metavar='NW',\n",
    "                    help='number of workers (default: 8)')\n",
    "parser.add_argument('--learning-rate', default=0.001, type=float, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--margin', default=0.5, type=float, metavar='MG',\n",
    "                    help='margin (default: 0.5)')\n",
    "parser.add_argument('--train-root-dir', default='/home/jupyter/data/face-image/training_aihub_sample/', type=str,\n",
    "                    help='path to train root dir')\n",
    "parser.add_argument('--valid-root-dir', default='/home/jupyter/data/face-image/training_aihub_sample/', type=str,\n",
    "                    help='path to valid root dir')\n",
    "parser.add_argument('--train-csv-name', default='datasets/dataset.csv', type=str,\n",
    "                    help='list of training images')\n",
    "parser.add_argument('--valid-csv-name', default='datasets/dataset.csv', type=str, help='list of validtion images')\n",
    "parser.add_argument('--step-size', default=50, type=int, metavar='SZ',\n",
    "                    help='Decay learning rate schedules every --step-size (default: 50)')\n",
    "parser.add_argument('--unfreeze', type=str, metavar='UF', default='',\n",
    "                    help='Provide an option for unfreezeing given layers')\n",
    "parser.add_argument('--freeze', type=str, metavar='F', default='',\n",
    "                    help='Provide an option for freezeing given layers')\n",
    "parser.add_argument('--pretrain', action='store_true')\n",
    "parser.add_argument('--fc-only', action='store_true')\n",
    "parser.add_argument('--except-fc', action='store_true')\n",
    "parser.add_argument('--load-best', action='store_true')\n",
    "parser.add_argument('--load-last', action='store_true')\n",
    "parser.add_argument('--continue-step', action='store_true')\n",
    "parser.add_argument('--train-all', action='store_true', help='Train all layers')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "#주피터에서 사용\n",
    "args = parser.parse_args(args=[])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "l2_dist = PairwiseDistance(2)\n",
    "modelsaver = ModelSaver()\n",
    "\n",
    "\n",
    "\n",
    "def save_if_best(state, acc):\n",
    "    modelsaver.save_if_best(acc, state)\n",
    "\n",
    "\n",
    "# main\n",
    "\n",
    "\n",
    "def save_last_checkpoint(state):\n",
    "    torch.save(state, 'log/last_checkpoint.pth')\n",
    "\n",
    "\n",
    "def train_valid(model, optimizer, triploss, scheduler, epoch, dataloaders, data_size):\n",
    "    for phase in ['train', 'valid']:\n",
    "\n",
    "        labels, distances = [], []\n",
    "        triplet_loss_sum = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            if scheduler.last_epoch % scheduler.step_size == 0:\n",
    "                print(\"LR decayed to:\", ', '.join(map(str, scheduler.get_lr())))\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for batch_idx, batch_sample in enumerate(dataloaders[phase]):\n",
    "\n",
    "            anc_img = batch_sample['anc_img'].to(device)\n",
    "            pos_img = batch_sample['pos_img'].to(device)\n",
    "            neg_img = batch_sample['neg_img'].to(device)\n",
    "\n",
    "            # pos_cls = batch_sample['pos_class'].to(device)\n",
    "            # neg_cls = batch_sample['neg_class'].to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                # anc_embed, pos_embed and neg_embed are encoding(embedding) of image\n",
    "                anc_embed, pos_embed, neg_embed = model(anc_img), model(pos_img), model(neg_img)\n",
    "\n",
    "                # choose the semi hard negatives only for \"training\"\n",
    "                pos_dist = l2_dist.forward(anc_embed, pos_embed)\n",
    "                neg_dist = l2_dist.forward(anc_embed, neg_embed)\n",
    "\n",
    "                all = (neg_dist - pos_dist < args.margin).cpu().numpy().flatten()\n",
    "                if phase == 'train':\n",
    "                    hard_triplets = np.where(all == 1)\n",
    "                    if len(hard_triplets[0]) == 0:\n",
    "                        continue\n",
    "                else:\n",
    "                    hard_triplets = np.where(all >= 0)\n",
    "\n",
    "                anc_hard_embed = anc_embed[hard_triplets]\n",
    "                pos_hard_embed = pos_embed[hard_triplets]\n",
    "                neg_hard_embed = neg_embed[hard_triplets]\n",
    "\n",
    "                anc_hard_img = anc_img[hard_triplets]\n",
    "                pos_hard_img = pos_img[hard_triplets]\n",
    "                neg_hard_img = neg_img[hard_triplets]\n",
    "\n",
    "                # pos_hard_cls = pos_cls[hard_triplets]\n",
    "                # neg_hard_cls = neg_cls[hard_triplets]\n",
    "\n",
    "                model.module.forward_classifier(anc_hard_img)\n",
    "                model.module.forward_classifier(pos_hard_img)\n",
    "                model.module.forward_classifier(neg_hard_img)\n",
    "                # model.classify(anc_hard_img)\n",
    "                # model.classify(pos_hard_img)\n",
    "                # model.classify(neg_hard_img)\n",
    "\n",
    "                triplet_loss = triploss.forward(anc_hard_embed, pos_hard_embed, neg_hard_embed)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    triplet_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                distances.append(pos_dist.data.cpu().numpy())\n",
    "                labels.append(np.ones(pos_dist.size(0)))\n",
    "\n",
    "                distances.append(neg_dist.data.cpu().numpy())\n",
    "                labels.append(np.zeros(neg_dist.size(0)))\n",
    "\n",
    "                triplet_loss_sum += triplet_loss.item()\n",
    "\n",
    "        avg_triplet_loss = triplet_loss_sum / data_size[phase]\n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "        tpr, fpr, accuracy, val, val_std, far = evaluate(distances, labels)\n",
    "        print('  {} set - Triplet Loss       = {:.8f}'.format(phase, avg_triplet_loss))\n",
    "        print('  {} set - Accuracy           = {:.8f}'.format(phase, np.mean(accuracy)))\n",
    "\n",
    "        time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        lr = '_'.join(map(str, scheduler.get_lr()))\n",
    "        layers = '+'.join(args.unfreeze.split(','))\n",
    "        write_csv(f'log/{phase}.csv', [time, epoch, np.mean(accuracy), avg_triplet_loss, layers, args.batch_size, lr])\n",
    "\n",
    "        if phase == 'valid':\n",
    "            save_last_checkpoint({'epoch': epoch,\n",
    "                                  'state_dict': model.module.state_dict(),\n",
    "                                  'optimizer_state': optimizer.state_dict(),\n",
    "                                  'accuracy': np.mean(accuracy),\n",
    "                                  'loss': avg_triplet_loss\n",
    "                                  })\n",
    "            save_if_best({'epoch': epoch,\n",
    "                          'state_dict': model.module.state_dict(),\n",
    "                          'optimizer_state': optimizer.state_dict(),\n",
    "                          'accuracy': np.mean(accuracy),\n",
    "                          'loss': avg_triplet_loss\n",
    "                          }, np.mean(accuracy))\n",
    "        else:\n",
    "            plot_roc(fpr, tpr, figure_name='./log/roc_valid_epoch_{}.png'.format(epoch))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    init_log_just_created(\"log/valid.csv\")\n",
    "    init_log_just_created(\"log/train.csv\")\n",
    "    import pandas as pd\n",
    "    valid = pd.read_csv('log/valid.csv')\n",
    "    max_acc = valid['acc'].max()\n",
    "\n",
    "    pretrain = args.pretrain\n",
    "    fc_only = args.fc_only\n",
    "    except_fc = args.except_fc\n",
    "    train_all = args.train_all\n",
    "    unfreeze = args.unfreeze.split(',')\n",
    "    freeze = args.freeze.split(',')\n",
    "    start_epoch = 0\n",
    "    print(f\"Transfer learning: {pretrain}\")\n",
    "    print(\"Train fc only:\", fc_only)\n",
    "    print(\"Train except fc:\", except_fc)\n",
    "    print(\"Train all layers:\", train_all)\n",
    "    print(\"Unfreeze only:\", ', '.join(unfreeze))\n",
    "    print(\"Freeze only:\", ', '.join(freeze))\n",
    "    print(f\"Max acc: {max_acc:.4f}\")\n",
    "    print(f\"Learning rate will decayed every {args.step_size}th epoch\")\n",
    "    \n",
    "    model = FaceNetModel()\n",
    "    model.to(device)\n",
    "    # model = InceptionResnetV1(classify=True, pretrained='vggface2').to(device)\n",
    "    triplet_loss = TripletLoss(args.margin).to(device)\n",
    "\n",
    "    # if fc_only:\n",
    "    #     model.unfreeze_only(['fc', 'classifier'])\n",
    "    # if except_fc:\n",
    "    #     model.freeze_only(['fc', 'classifier'])\n",
    "    # if train_all:\n",
    "    #     model.unfreeze_all()\n",
    "    # if len(unfreeze) > 0:\n",
    "    #     model.unfreeze_only(unfreeze)\n",
    "    # if len(freeze) > 0:\n",
    "    #     model.freeze_only(freeze)\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=0.1)\n",
    "\n",
    "    if args.load_best or args.load_last:\n",
    "        checkpoint = './log/best_state.pth' if args.load_best else './log/last_checkpoint.pth'\n",
    "        print('loading', checkpoint)\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        modelsaver.current_acc = max_acc\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"Stepping scheduler\")\n",
    "        try:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        except ValueError as e:\n",
    "            print(\"Can't load last optimizer\")\n",
    "            print(e)\n",
    "        if args.continue_step:\n",
    "            scheduler.step(checkpoint['epoch'])\n",
    "        print(f\"Loaded checkpoint epoch: {checkpoint['epoch']}\\n\"\n",
    "              f\"Loaded checkpoint accuracy: {checkpoint['accuracy']}\\n\"\n",
    "              f\"Loaded checkpoint loss: {checkpoint['loss']}\")\n",
    "\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    for epoch in range(start_epoch, args.num_epochs + start_epoch):\n",
    "        print(80 * '=')\n",
    "        print('Epoch [{}/{}]'.format(epoch, args.num_epochs + start_epoch - 1))\n",
    "\n",
    "        time0 = time.time()\n",
    "        data_loaders, data_size = get_dataloader(args.train_root_dir, args.valid_root_dir,\n",
    "                                                 args.train_csv_name, args.valid_csv_name,\n",
    "                                                 args.num_train_triplets, args.num_valid_triplets,\n",
    "                                                 args.batch_size, args.num_workers)\n",
    "\n",
    "        train_valid(model, optimizer, triplet_loss, scheduler, epoch, data_loaders, data_size)\n",
    "        print(f'  Execution time                 = {time.time() - time0}')\n",
    "    print(80 * '=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning: False\n",
      "Train fc only: False\n",
      "Train except fc: False\n",
      "Train all layers: False\n",
      "Unfreeze only: \n",
      "Freeze only: \n",
      "Max acc: 0.5700\n",
      "Learning rate will decayed every 50th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:136: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
      "  f\"Using {sequence_to_str(tuple(keyword_only_kwargs.keys()), separate_last='and ')} as positional \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Epoch [0/49]\n",
      "  train set - Triplet Loss       = 0.06229981\n",
      "  train set - Accuracy           = 0.40500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  valid set - Triplet Loss       = 0.03767534\n",
      "  valid set - Accuracy           = 0.53000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Execution time                 = 30.05941915512085\n",
      "================================================================================\n",
      "Epoch [1/49]\n",
      "  train set - Triplet Loss       = 0.04447251\n",
      "  train set - Accuracy           = 0.33500000\n",
      "  valid set - Triplet Loss       = 0.04390312\n",
      "  valid set - Accuracy           = 0.30500000\n",
      "  Execution time                 = 34.40707516670227\n",
      "================================================================================\n",
      "Epoch [2/49]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14116/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14116/2455230426.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                  args.batch_size, args.num_workers)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtrain_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  Execution time                 = {time.time() - time0}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14116/47351429.py\u001b[0m in \u001b[0;36mtrain_valid\u001b[0;34m(model, optimizer, triploss, scheduler, epoch, dataloaders, data_size)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubdist\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  {} set - Triplet Loss       = {:.8f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_triplet_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  {} set - Accuracy           = {:.8f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/ming-fpt/ming/facenet/eval_metrics.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(distances, labels, nrof_folds)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     val, val_std, far = calculate_val(thresholds, distances,\n\u001b[0;32m---> 13\u001b[0;31m                                       labels, 1e-3, nrof_folds=nrof_folds)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/ming-fpt/ming/facenet/eval_metrics.py\u001b[0m in \u001b[0;36mcalculate_val\u001b[0;34m(thresholds, distances, labels, far_target, nrof_folds)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mfar_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrof_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthreshold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfar_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthreshold_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_val_far\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfar_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mfar_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfar_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'slinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/ming-fpt/ming/facenet/eval_metrics.py\u001b[0m in \u001b[0;36mcalculate_val_far\u001b[0;34m(threshold, dist, actual_issame)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mfalse_accept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_issame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mn_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_issame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_diff\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA out of memory  \n",
    "먼저 nvidia-smi로 백그라운드에서 실행중인 프로세스가 있는지 확인합니다.\n",
    "\n",
    "아래 화면에서는 No running processes found라고 뜨는데요, 만약 백그라운드에서 실행중인 프로세스가 있다면 실행중인 프로세스의 목록이 뜨게 됩니다.  \n",
    "해당 프로세스들이 GPU Memory Usage를 사용하고 있기 때문에, cuda out of memory 에러가 발행하는 것인데요, 만약 현재 실행하려는 프로세스가 아니라면, PID를 이용하여 프로세스를 kill해줍니다. 가끔 서버가 비정상적으로 종료된 경우, 프로세스가 정상적으로 종료되지 않고 백그라운드에 남을 수 있습니다.  \n",
    "\n",
    "kill -15 [PID]  \n",
    "만약 이 코드로도 프로세스가 종료되지 않는다면, 아래의 코드를 실행해주세요.\n",
    "\n",
    "kill -9 [PID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 31 06:30:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0    28W /  70W |   7077MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     14116      C   python                           7075MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 (default, Oct 31 2022, 14:04:00) \n[GCC 8.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
